#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script to train the AlterMoveProbNN model using data generated by create_data.py.

This script loads the CSV data, processes it, and trains the AlterMoveProbNN model.
"""

import os
import sys
import ast
import glob
import chess
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import train_test_split

# Add the main directory to the path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from development.alter_move_prob_train.alter_move_prob_nn import AlterMoveProbNN


class AlterMoveProbDataset(Dataset):
    """
    Dataset for training the AlterMoveProbNN model.
    """
    def __init__(self, csv_file):
        """
        Initialize the dataset.
        
        Args:
            csv_file: Path to the CSV file containing training data
        """
        self.data = pd.read_csv(csv_file)
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        Get a training sample.
        
        Args:
            idx: Index of the sample
            
        Returns:
            tuple: (move_dic, board, prev_board, prev_prev_board, true_move)
        """
        row = self.data.iloc[idx]
        
        try:
            # Parse the move dictionary with additional safety checks
            try:
                move_dic_str = row['move_dic']
                if pd.isna(move_dic_str) or not isinstance(move_dic_str, str):
                    return None, None, None, None, None
                
                move_dic = ast.literal_eval(move_dic_str)
                # Validate move_dic is a proper dictionary with valid probabilities
                if not isinstance(move_dic, dict) or not move_dic:
                    return None, None, None, None, None
                
                # Ensure all probabilities are valid
                for k, v in list(move_dic.items()):
                    if not isinstance(v, (int, float)) or v < 0:
                        move_dic[k] = 1e-8
            except (SyntaxError, ValueError) as e:
                print(f"Error parsing move_dic at index {idx}: {e}")
                return None, None, None, None, None
            
            # Create board objects from FEN strings
            board = None
            if not pd.isna(row['fen']) and isinstance(row['fen'], str):
                try:
                    board = chess.Board(row['fen'])
                except ValueError as e:
                    print(f"Invalid FEN at index {idx}: {e}")
                    return None, None, None, None, None
            
            if board is None:
                return None, None, None, None, None
            
            prev_board = None
            if not pd.isna(row['prev_fen']) and isinstance(row['prev_fen'], str):
                try:
                    prev_board = chess.Board(row['prev_fen'])
                except ValueError:
                    prev_board = None
            
            prev_prev_board = None
            if not pd.isna(row['prev_prev_fen']) and prev_board is not None and isinstance(row['prev_prev_fen'], str):
                try:
                    prev_prev_board = chess.Board(row['prev_prev_fen'])
                except ValueError:
                    prev_prev_board = None
            
            # Get the true move (the move that was actually played)
            true_move = row['true_move']
            if pd.isna(true_move) or not isinstance(true_move, str) or not true_move:
                return None, None, None, None, None
                
            # Validate true_move is a valid UCI move
            try:
                chess.Move.from_uci(true_move)
            except ValueError:
                return None, None, None, None, None
            
            return move_dic, board, prev_board, prev_prev_board, true_move
        except Exception as e:
            print(f"Error processing data at index {idx}: {e}")
            return None, None, None, None, None


def train_model(data_file='development/alter_move_prob_train/data/training_data.csv', batch_size=32, epochs=50, learning_rate=0.001, 
                save_path='development/alter_move_prob_train/data/alter_move_prob_nn.pth', rank_cutoff=3):
    """
    Train the AlterMoveProbNN model.
    
    Args:
        data_file: Path to the CSV file containing training data
        batch_size: Batch size for training
        epochs: Number of training epochs
        learning_rate: Learning rate for the optimizer
        save_path: Path to save the trained model
        rank_cutoff: Cutoff rank for the loss function (default: 3)
    """
    # Create the model
    model = AlterMoveProbNN()

    # Load pretrained weights if they exist
    best_model_path = 'development/alter_move_prob_train/data/alter_move_prob_nn_best.pth'
    if os.path.exists(best_model_path):
        try:
            model.load_state_dict(torch.load(best_model_path, weights_only=True))
            print(f"Loaded pretrained weights from {best_model_path}")
        except Exception as e:
            print(f"Error loading pretrained weights: {e}")
            print("Training with fresh model weights")
        
        # model.repeat_sf = nn.Parameter(torch.tensor(0.0, dtype=torch.float))
    else:
        print("No pretrained weights found. Training with fresh model weights")
    
    # Create the optimizer directly on the model parameters
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # Create a data loader
    data_loader = DataLoader(AlterMoveProbDataset(data_file), batch_size=batch_size, shuffle=True, 
                            num_workers=4, collate_fn=lambda x: x)
    
    # Define ELU activation for the loss function
    elu = nn.ReLU()
    
    # Training loop
    best_loss = float('inf')
    best_model_state = None
    
    for epoch in range(epochs):
        total_loss = 0.0
        valid_samples = 0
        
        # Get initial parameter values for tqdm display
        param_values = {name: param.item() for name, param in model.named_parameters()}
        
        # Create progress bar with parameter tracking
        progress_bar = tqdm(data_loader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch_idx, batch in enumerate(progress_bar):
            batch_loss = 0.0
            valid_batch_samples = 0
            
            # Process each item in the batch
            for move_dic, board, prev_board, prev_prev_board, true_move in batch:
                # Skip invalid samples
                if board is None or move_dic is None or true_move is None:
                    continue
                
                # Zero the gradients
                optimizer.zero_grad()
                
                try:
                    # Forward pass through the model
                    altered_move_dic, _ = model(move_dic, board, prev_board, prev_prev_board)
                    
                    # Ensure all probabilities are tensors with gradient tracking
                    for move in altered_move_dic:
                        if not isinstance(altered_move_dic[move], torch.Tensor):
                            altered_move_dic[move] = torch.tensor(altered_move_dic[move], dtype=torch.float)
                        # Clamp probabilities to avoid numerical issues
                        altered_move_dic[move] = torch.clamp(altered_move_dic[move], min=1e-8, max=1.0)
                    
                    # Calculate loss using the new loss function
                    if true_move in altered_move_dic:
                        true_move_prob = altered_move_dic[true_move]
                        
                        # Sort moves by probabilities in descending order
                        sorted_probs = sorted([altered_move_dic[m] for m in altered_move_dic], reverse=True)
                        
                        # Get the rank_cutoff-th probability (if available)
                        cutoff_prob = sorted_probs[min(rank_cutoff-1, len(sorted_probs)-1)]
                        
                        # Calculate the difference between true move probability and cutoff probability
                        prob_diff = cutoff_prob.clone().detach() - true_move_prob
                        
                        # combination of relu and log loss
                        loss = 5*elu(prob_diff) - torch.log(true_move_prob)
                        
                        # Check for NaN or inf
                        if torch.isnan(loss) or torch.isinf(loss):
                            print(f"Warning: NaN/Inf loss detected. true_move_prob = {true_move_prob.item()}, cutoff_prob = {cutoff_prob.item()}")
                            # Skip this sample
                            continue
                    else:
                        # If the true move is not in the altered move dictionary, assign a high loss
                        loss = torch.tensor(10.0, dtype=torch.float)
                    
                    # Backward pass
                    loss.backward()
                    
                    # Check for NaN gradients and clip if necessary
                    for name, param in model.named_parameters():
                        if param.grad is not None:
                            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                                print(f"Warning: NaN/Inf gradient detected in {name}. Clipping.")
                                param.grad.data = torch.zeros_like(param.grad.data)
                            else:
                                # Clip gradients to prevent exploding gradients
                                torch.nn.utils.clip_grad_norm_(param, 1.0)
                    
                    # Update parameters
                    optimizer.step()
                    
                    # Add to batch loss and count valid samples
                    batch_loss += loss.item()
                    valid_batch_samples += 1
                    
                    # Update parameter values for tqdm display
                    param_values = {name: param.item() for name, param in model.named_parameters()}
                    
                except Exception as e:
                    print(f"Error processing sample: {e}")
                    continue
            
            # Average the batch loss (if there were valid samples)
            if valid_batch_samples > 0:
                batch_loss = batch_loss / valid_batch_samples
                total_loss += batch_loss
                valid_samples += 1
                
                # Update the progress bar with loss and parameter values
                # Create a compact postfix with just the essential information
                postfix_dict = {'loss': f"{batch_loss:.4f}"}
                
                # Add only the most important parameters to avoid progress bar overflow
                important_params = {k: v for i, (k, v) in enumerate(param_values.items()) 
                                   if i < 15}  # Limit to first 3 parameters
                
                for name, value in important_params.items():
                    short_name = name.split('.')[-1][:2]  # Use shortened parameter names
                    postfix_dict[short_name] = f"{value:.1f}"  # Use fewer decimal places
                
                progress_bar.set_postfix(**postfix_dict)
                
                # Print parameter values every 100 batches
                if (batch_idx + 1) % 100 == 0:
                    print(f"\nCurrent parameter values after batch {batch_idx + 1}:")
                    for name, param in model.named_parameters():
                        print(f"{name}: {param.item():.4f}")
                    print(f"Current average loss: {total_loss / valid_samples:.4f}")
            else:
                progress_bar.set_postfix(loss="N/A", samples=0)
        
        # Print the average loss for the epoch
        if valid_samples > 0:
            avg_loss = total_loss / valid_samples
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Valid batches: {valid_samples}/{len(data_loader)}")
            
            # Print model parameters after each epoch
            print(f"\nModel parameters after epoch {epoch+1}:")
            for name, param in model.named_parameters():
                print(f"{name}: {param.item():.4f}")
            
            # Save the best model
            if avg_loss < best_loss:
                best_loss = avg_loss
                best_model_state = model.state_dict().copy()
                # Save the best model so far
                torch.save(best_model_state, save_path.replace('.pth', '_best.pth'))
                print(f"New best model saved with loss: {best_loss:.4f}")
        else:
            print(f"Epoch {epoch+1}/{epochs}, No valid samples processed")
    
    # Save the final model
    torch.save(model.state_dict(), save_path)
    print(f"Final model saved to {save_path}")
    
    # If we have a best model, save it again to ensure we have the best version
    if best_model_state is not None:
        # Load the best model state
        model.load_state_dict(best_model_state)
        torch.save(model.state_dict(), save_path.replace('.pth', '_best.pth'))
        print(f"Best model saved to {save_path.replace('.pth', '_best.pth')} with loss: {best_loss:.4f}")
    
    # Print the final model parameters
    print("\nFinal model parameters:")
    for name, param in model.named_parameters():
        print(f"{name}: {param.item():.4f}")


def main():
    """
    Main function to train the AlterMoveProbNN model.
    """
    # Load all training data files
    os.makedirs('development/alter_move_prob_train/data', exist_ok=True)
    
    # Look for all training data files
    data_files = glob.glob('development/alter_move_prob_train/data/training_data_*.csv')
    if not data_files:
        print("No training data found. Please run create_data.py first.")
        return
    
    latest_data_file = max(data_files, key=os.path.getctime)
    print(f"Using training data from {latest_data_file}")
    
    # Train the model with default rank_cutoff=3
    train_model(data_file=latest_data_file, epochs=50, rank_cutoff=3, learning_rate=0.00005)


if __name__ == "__main__":
    main() 